{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Open Source Analysis\n",
    "\n",
    "This project analyzes Amazon's Free and Open Source Software (FOSS) Contributions to\n",
    "determine the extent to which they behave as good citizens of the open source community.\n",
    "Much has been said about Amazon and open source software, much of it good from Amazon and\n",
    "much of it bad from the community. But what is the reality? Numbers that tend to indicate\n",
    "good citizenship are cited by Amazon, Google, Microsoft and other cloud providers but they\n",
    "are never checked for accuracy, honesty and integrity. This project aims to change that by\n",
    "rigorously evaluating the open source contributions of cloud providers, starting with Amazon.\n",
    "\n",
    "This project is an attempt to check the veracity of the claim made by Deirdré Straughan, managing editor of the AWS Open Source blog, on April 8, 2019 that, \"Amazon has contributed [over 1,800 projects](https://github.com/search?utf8=%E2%9C%93&q=+user%3Aalexa+user%3Aamzn+user%3Aaws+user%3Aawsdocs+user%3Aawslabs+user%3Aaws-quickstart+user%3Ablox+user%3Aboto+user%3Ac9+user%3Acorretto+user%3Afirecracker-microvm+user%3Aaws-robotics+user%3Aajaxorg+user%3Agluon-api+user%3Acloud9ide+user%3ACarbonado+user%3Agoodreads+user%3AIvonaSoftware+user%3Atwitchtv+user%3Atwitchdev+user%3Atwitchscience+user%3Ajustintv+user%3AZappos+user%3Aamazon-archives+user%3Aalexa-labs+user%3Aaws-samples+user%3Aaws-amplify+user%3Aaws-cloudformation+user%3Aaws-solutions+user%3Aopendistro-for-elasticsearch+user%3Aopendistro&type=Repositories&ref=advsearch&l=&l=)\n",
    "across 30 GitHub organizations ranging from [Alexa](https://github.com/alexa) to\n",
    "[Zappos](https://github.com/Zappos). You can search them all from\n",
    "[aws.github.com](https://aws.github.io/).\"\n",
    "\n",
    "## Source: Snorkel Tutorials\n",
    "\n",
    "This project uses code from the [Snorkel Tutorials](https://github.com/snorkel-team/snorkel-tutorials) at [https://github.com/snorkel-team/snorkel-tutorials](https://github.com/snorkel-team/snorkel-tutorials)\n",
    "\n",
    "Thanks to the [Snorkel Team](https://www.snorkel.org/) for making this important work feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Make sure randomness is reproducible\n",
    "import random\n",
    "random.seed('31337')\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total repositories: 2,568\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>private</th>\n",
       "      <th>owner</th>\n",
       "      <th>html_url</th>\n",
       "      <th>description</th>\n",
       "      <th>fork</th>\n",
       "      <th>url</th>\n",
       "      <th>...</th>\n",
       "      <th>archived</th>\n",
       "      <th>disabled</th>\n",
       "      <th>open_issues_count</th>\n",
       "      <th>license</th>\n",
       "      <th>forks</th>\n",
       "      <th>open_issues</th>\n",
       "      <th>watchers</th>\n",
       "      <th>default_branch</th>\n",
       "      <th>permissions</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61861755</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk2MTg2MTc1NQ==</td>\n",
       "      <td>alexa-skills-kit-sdk-for-nodejs</td>\n",
       "      <td>alexa/alexa-skills-kit-sdk-for-nodejs</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'alexa', 'id': 17815977, 'node_id': ...</td>\n",
       "      <td>https://github.com/alexa/alexa-skills-kit-sdk-...</td>\n",
       "      <td>The Alexa Skills Kit SDK for Node.js helps you...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/alexa/alexa-skill...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>{'key': 'apache-2.0', 'name': 'Apache License ...</td>\n",
       "      <td>670</td>\n",
       "      <td>8</td>\n",
       "      <td>2811</td>\n",
       "      <td>2.0.x</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84138837</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk4NDEzODgzNw==</td>\n",
       "      <td>alexa-cookbook</td>\n",
       "      <td>alexa/alexa-cookbook</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'alexa', 'id': 17815977, 'node_id': ...</td>\n",
       "      <td>https://github.com/alexa/alexa-cookbook</td>\n",
       "      <td>A series of sample code projects to be used fo...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/alexa/alexa-cookbook</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>{'key': 'other', 'name': 'Other', 'spdx_id': '...</td>\n",
       "      <td>912</td>\n",
       "      <td>13</td>\n",
       "      <td>1557</td>\n",
       "      <td>master</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63275452</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk2MzI3NTQ1Mg==</td>\n",
       "      <td>skill-sample-nodejs-fact</td>\n",
       "      <td>alexa/skill-sample-nodejs-fact</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'alexa', 'id': 17815977, 'node_id': ...</td>\n",
       "      <td>https://github.com/alexa/skill-sample-nodejs-fact</td>\n",
       "      <td>Build An Alexa Fact Skill</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/alexa/skill-sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>{'key': 'apache-2.0', 'name': 'Apache License ...</td>\n",
       "      <td>1186</td>\n",
       "      <td>7</td>\n",
       "      <td>1002</td>\n",
       "      <td>master</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                           node_id  \\\n",
       "0  61861755  MDEwOlJlcG9zaXRvcnk2MTg2MTc1NQ==   \n",
       "1  84138837  MDEwOlJlcG9zaXRvcnk4NDEzODgzNw==   \n",
       "2  63275452  MDEwOlJlcG9zaXRvcnk2MzI3NTQ1Mg==   \n",
       "\n",
       "                              name                              full_name  \\\n",
       "0  alexa-skills-kit-sdk-for-nodejs  alexa/alexa-skills-kit-sdk-for-nodejs   \n",
       "1                   alexa-cookbook                   alexa/alexa-cookbook   \n",
       "2         skill-sample-nodejs-fact         alexa/skill-sample-nodejs-fact   \n",
       "\n",
       "   private                                              owner  \\\n",
       "0    False  {'login': 'alexa', 'id': 17815977, 'node_id': ...   \n",
       "1    False  {'login': 'alexa', 'id': 17815977, 'node_id': ...   \n",
       "2    False  {'login': 'alexa', 'id': 17815977, 'node_id': ...   \n",
       "\n",
       "                                            html_url  \\\n",
       "0  https://github.com/alexa/alexa-skills-kit-sdk-...   \n",
       "1            https://github.com/alexa/alexa-cookbook   \n",
       "2  https://github.com/alexa/skill-sample-nodejs-fact   \n",
       "\n",
       "                                         description   fork  \\\n",
       "0  The Alexa Skills Kit SDK for Node.js helps you...  False   \n",
       "1  A series of sample code projects to be used fo...  False   \n",
       "2                          Build An Alexa Fact Skill  False   \n",
       "\n",
       "                                                 url  ... archived disabled  \\\n",
       "0  https://api.github.com/repos/alexa/alexa-skill...  ...    False    False   \n",
       "1  https://api.github.com/repos/alexa/alexa-cookbook  ...    False    False   \n",
       "2  https://api.github.com/repos/alexa/skill-sampl...  ...    False    False   \n",
       "\n",
       "  open_issues_count                                            license forks  \\\n",
       "0                 8  {'key': 'apache-2.0', 'name': 'Apache License ...   670   \n",
       "1                13  {'key': 'other', 'name': 'Other', 'spdx_id': '...   912   \n",
       "2                 7  {'key': 'apache-2.0', 'name': 'Apache License ...  1186   \n",
       "\n",
       "  open_issues watchers default_branch  \\\n",
       "0           8     2811          2.0.x   \n",
       "1          13     1557         master   \n",
       "2           7     1002         master   \n",
       "\n",
       "                                     permissions score  \n",
       "0  {'admin': False, 'push': False, 'pull': True}     1  \n",
       "1  {'admin': False, 'push': False, 'pull': True}     1  \n",
       "2  {'admin': False, 'push': False, 'pull': True}     1  \n",
       "\n",
       "[3 rows x 75 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the AWS Github repo READMEs scraped from Github\n",
    "df = pd.read_json('data/aws_repos.jsonl', lines=True)\n",
    "print(f'Total repositories: {len(df.index):,}')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many are original projects and how many are forks?\n",
    "\n",
    "If the `fork` field is True, this isn't an Amazon company project - it is a fork of another project. Let's see how many they've created and how many their companies have forked. Forking a project is not an indicator of contributing a project - it is one click in Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original repositories: 2568\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total original repositories: {len(df[df['fork'] == False].index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>private</th>\n",
       "      <th>owner</th>\n",
       "      <th>html_url</th>\n",
       "      <th>description</th>\n",
       "      <th>fork</th>\n",
       "      <th>url</th>\n",
       "      <th>...</th>\n",
       "      <th>archived</th>\n",
       "      <th>disabled</th>\n",
       "      <th>open_issues_count</th>\n",
       "      <th>license</th>\n",
       "      <th>forks</th>\n",
       "      <th>open_issues</th>\n",
       "      <th>watchers</th>\n",
       "      <th>default_branch</th>\n",
       "      <th>permissions</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>10943591</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnkxMDk0MzU5MQ==</td>\n",
       "      <td>dynamodb-transactions</td>\n",
       "      <td>awslabs/dynamodb-transactions</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'awslabs', 'id': 3299148, 'node_id':...</td>\n",
       "      <td>https://github.com/awslabs/dynamodb-transactions</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/awslabs/dynamodb-...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>{'key': 'apache-2.0', 'name': 'Apache License ...</td>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>303</td>\n",
       "      <td>master</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                           node_id                   name  \\\n",
       "660  10943591  MDEwOlJlcG9zaXRvcnkxMDk0MzU5MQ==  dynamodb-transactions   \n",
       "\n",
       "                         full_name  private  \\\n",
       "660  awslabs/dynamodb-transactions    False   \n",
       "\n",
       "                                                 owner  \\\n",
       "660  {'login': 'awslabs', 'id': 3299148, 'node_id':...   \n",
       "\n",
       "                                             html_url description   fork  \\\n",
       "660  https://github.com/awslabs/dynamodb-transactions        None  False   \n",
       "\n",
       "                                                   url  ... archived disabled  \\\n",
       "660  https://api.github.com/repos/awslabs/dynamodb-...  ...    False    False   \n",
       "\n",
       "    open_issues_count                                            license  \\\n",
       "660                 4  {'key': 'apache-2.0', 'name': 'Apache License ...   \n",
       "\n",
       "    forks open_issues watchers default_branch  \\\n",
       "660    87           4      303         master   \n",
       "\n",
       "                                       permissions score  \n",
       "660  {'admin': False, 'push': False, 'pull': True}     1  \n",
       "\n",
       "[1 rows x 75 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['full_name'] == 'awslabs/dynamodb-transactions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like all the repositories listed were created by Amazon or by companies Amazon has acquired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get READMEs for each repository\n",
    "\n",
    "The [Github README API](https://developer.github.com/v3/repos/contents/#get-the-readme) makes it very easy to download the README of a project. Let's fetch the README of every Amazon open source project on Github.\n",
    "\n",
    "**Note: you need only run the following four code cells once, thereafter skip ahead to the 4th cell.**\n",
    "\n",
    "### Pull Github Tokens from Environment\n",
    "\n",
    "You need to have set the `GITHUB_USERNAME` and `GITHUB_TOKEN` environment variables in the shell from which you ran `docker-compose up` or `jupyter notebook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "# Ensure that both essential environment variables have been set.\n",
    "GITHUB_USERNAME = os.environ.get('GITHUB_USERNAME')\n",
    "if not GITHUB_USERNAME:\n",
    "    raise Exception('Environment variable GITHUB_USERNAME must be defined')\n",
    "\n",
    "GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')\n",
    "if not GITHUB_TOKEN:\n",
    "    raise Exception('Environment variable GITHUB_TOKEN must be defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a `get_readme(full_name)` Function\n",
    "\n",
    "We need a function `get_readme(full_name)` that will take a `full_name` and return a UTF-8 encoded Github README using the credentials set in the `GITHUB_USERNAME`/`GITHUB_TOKEN` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from urllib3.exceptions import ProtocolError\n",
    "\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError, SSLError, Timeout\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    'Accept-Encoding' : 'gzip'\n",
    "}\n",
    "GITHUB_TOKEN = os.environ.get('GITHUB_TOKEN')\n",
    "GITHUB_API = 'https://api.github.com/repos/{full_name}/readme'\n",
    "TIMEOUT = 3.0\n",
    "SLEEP_MINUTES = 11\n",
    "\n",
    "\n",
    "class GithubRequestException(Exception):\n",
    "    \"\"\"Set the status code of a Github request for evaluating exceptions\"\"\"\n",
    "    def __init__(self, message, status_code=None):\n",
    "        self.message = message\n",
    "        self.status_code = status_code\n",
    "\n",
    "\n",
    "def get_readme(full_name):\n",
    "    \"\"\"Given the full name of a project, return a UTF-8 README or throw a GithubRequestException\"\"\"\n",
    "    \n",
    "    api_url = GITHUB_API.format(\n",
    "        full_name=full_name,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(\n",
    "            api_url,\n",
    "            auth=(GITHUB_USERNAME, GITHUB_TOKEN),\n",
    "            timeout=TIMEOUT,\n",
    "            headers=HEADERS,\n",
    "        )\n",
    "        \n",
    "        # Parse the README\n",
    "        record = response.json()\n",
    "        readme_64 = record.get('content')\n",
    "        readme = None\n",
    "        if readme_64:\n",
    "            readme = base64.b64decode(\n",
    "                readme_64\n",
    "            ).decode()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return readme\n",
    "        else:\n",
    "            raise GithubRequestException(\n",
    "                f'Error: got response code {response.status_code}',\n",
    "                response.status_code\n",
    "            )\n",
    "\n",
    "    except SSLError:\n",
    "        raise GithubRequestException(\n",
    "            'SSL Error',\n",
    "            response.status_code\n",
    "        )\n",
    "\n",
    "    except Timeout:\n",
    "        raise GithubRequestException(\n",
    "            f'Timeout error: {TIMEOUT}s exceeded',\n",
    "            response.status_code\n",
    "        )\n",
    "    \n",
    "    except (ConnectionError, ProtocolError):\n",
    "        raise GithubRequestException(\n",
    "            'Connection or protocol error',\n",
    "            response.status_code\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables for READMEs\n",
    "\n",
    "We want these defined in a separate cell in case we have to retry our requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dict to hold READMEs so we can see if they're done already, but also a list to create a Series from\n",
    "readmes = {}\n",
    "readme_nums = {}\n",
    "readme_list = []\n",
    "\n",
    "# Use a dict to hold do-over flags so we don't redo work\n",
    "redos = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch READMEs for All Packages\n",
    "\n",
    "Now we loop through the `full_names` of all packages and store their READMEs in a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d827281a2f654697b2c92ef1d3251bc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Fetching READMEs for all packages', max=2568.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n",
      "ERROR:root:('Error: got response code 404', 404)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Need to redo 29 repos ...\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Materialize the list\n",
    "full_names = list(df['full_name'].iteritems())\n",
    "\n",
    "# Given a project full_name (owner/repo) fetch the README\n",
    "for i, full_name in tqdm(\n",
    "    full_names,\n",
    "    desc='Fetching READMEs for all packages'\n",
    "):\n",
    "    # We already did this one\n",
    "    if full_name in readmes and readmes[full_name] and isinstance(readmes[full_name], dict):\n",
    "        continue\n",
    "    \n",
    "    # Fetch the Github README, handling errors, logging any GithubRequestException\n",
    "    # for doing over again\n",
    "    try:\n",
    "        readme = get_readme(full_name)\n",
    "\n",
    "        # If we got a valid record back, insert it\n",
    "        if readme and isinstance(readme, str):\n",
    "            readmes[full_name] = readme\n",
    "            readme_nums[i] = readme\n",
    "            readme_list.append(readme)\n",
    "            # Get rid of any redo record\n",
    "            if full_name in redos:\n",
    "                del redos[full_name]\n",
    "        # Otherwise redo it and store empty string\n",
    "        else:\n",
    "            redos[full_name] = True\n",
    "            readme_list.append(readme)\n",
    "\n",
    "        # Don't flood Github\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    except GithubRequestException as e:\n",
    "        logging.error(e)\n",
    "        redos[full_name] = True\n",
    "        \n",
    "        if e.status_code == '403':\n",
    "            sleep_time = SLEEP_MINUTES * 60\n",
    "            logging.info(f'Ran into response code {e.status_code}! Sleeping for {sleep_time} minutes ...')\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "print(f'Need to redo {len(redos)} repos ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>node_id</th>\n",
       "      <th>name</th>\n",
       "      <th>full_name</th>\n",
       "      <th>private</th>\n",
       "      <th>owner</th>\n",
       "      <th>html_url</th>\n",
       "      <th>description</th>\n",
       "      <th>fork</th>\n",
       "      <th>url</th>\n",
       "      <th>...</th>\n",
       "      <th>disabled</th>\n",
       "      <th>open_issues_count</th>\n",
       "      <th>license</th>\n",
       "      <th>forks</th>\n",
       "      <th>open_issues</th>\n",
       "      <th>watchers</th>\n",
       "      <th>default_branch</th>\n",
       "      <th>permissions</th>\n",
       "      <th>score</th>\n",
       "      <th>readme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61861755</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk2MTg2MTc1NQ==</td>\n",
       "      <td>alexa-skills-kit-sdk-for-nodejs</td>\n",
       "      <td>alexa/alexa-skills-kit-sdk-for-nodejs</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'alexa', 'id': 17815977, 'node_id': ...</td>\n",
       "      <td>https://github.com/alexa/alexa-skills-kit-sdk-...</td>\n",
       "      <td>The Alexa Skills Kit SDK for Node.js helps you...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/alexa/alexa-skill...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>{'key': 'apache-2.0', 'name': 'Apache License ...</td>\n",
       "      <td>670</td>\n",
       "      <td>8</td>\n",
       "      <td>2811</td>\n",
       "      <td>2.0.x</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;img src=\"https://m.medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>84138837</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk4NDEzODgzNw==</td>\n",
       "      <td>alexa-cookbook</td>\n",
       "      <td>alexa/alexa-cookbook</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'alexa', 'id': 17815977, 'node_id': ...</td>\n",
       "      <td>https://github.com/alexa/alexa-cookbook</td>\n",
       "      <td>A series of sample code projects to be used fo...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/alexa/alexa-cookbook</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>{'key': 'other', 'name': 'Other', 'spdx_id': '...</td>\n",
       "      <td>912</td>\n",
       "      <td>13</td>\n",
       "      <td>1557</td>\n",
       "      <td>master</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "      <td>\\n# Alexa Skill Building Cookbook\\n\\n&lt;div styl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63275452</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk2MzI3NTQ1Mg==</td>\n",
       "      <td>skill-sample-nodejs-fact</td>\n",
       "      <td>alexa/skill-sample-nodejs-fact</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'alexa', 'id': 17815977, 'node_id': ...</td>\n",
       "      <td>https://github.com/alexa/skill-sample-nodejs-fact</td>\n",
       "      <td>Build An Alexa Fact Skill</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/alexa/skill-sampl...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>{'key': 'apache-2.0', 'name': 'Apache License ...</td>\n",
       "      <td>1186</td>\n",
       "      <td>7</td>\n",
       "      <td>1002</td>\n",
       "      <td>master</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "      <td># Build An Alexa Fact Skill\\n&lt;img src=\"https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81483877</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnk4MTQ4Mzg3Nw==</td>\n",
       "      <td>avs-device-sdk</td>\n",
       "      <td>alexa/avs-device-sdk</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'alexa', 'id': 17815977, 'node_id': ...</td>\n",
       "      <td>https://github.com/alexa/avs-device-sdk</td>\n",
       "      <td>An SDK for commercial device makers to integra...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/alexa/avs-device-sdk</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>54</td>\n",
       "      <td>{'key': 'apache-2.0', 'name': 'Apache License ...</td>\n",
       "      <td>477</td>\n",
       "      <td>54</td>\n",
       "      <td>993</td>\n",
       "      <td>master</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "      <td>### What is the Alexa Voice Service (AVS)?\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38904647</td>\n",
       "      <td>MDEwOlJlcG9zaXRvcnkzODkwNDY0Nw==</td>\n",
       "      <td>alexa-skills-kit-sdk-for-java</td>\n",
       "      <td>alexa/alexa-skills-kit-sdk-for-java</td>\n",
       "      <td>False</td>\n",
       "      <td>{'login': 'alexa', 'id': 17815977, 'node_id': ...</td>\n",
       "      <td>https://github.com/alexa/alexa-skills-kit-sdk-...</td>\n",
       "      <td>The Alexa Skills Kit SDK for Java helps you ge...</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.github.com/repos/alexa/alexa-skill...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>{'key': 'apache-2.0', 'name': 'Apache License ...</td>\n",
       "      <td>720</td>\n",
       "      <td>2</td>\n",
       "      <td>715</td>\n",
       "      <td>2.0.x</td>\n",
       "      <td>{'admin': False, 'push': False, 'pull': True}</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;p align=\"center\"&gt;\\n  &lt;img src=\"https://m.medi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                           node_id  \\\n",
       "0  61861755  MDEwOlJlcG9zaXRvcnk2MTg2MTc1NQ==   \n",
       "1  84138837  MDEwOlJlcG9zaXRvcnk4NDEzODgzNw==   \n",
       "2  63275452  MDEwOlJlcG9zaXRvcnk2MzI3NTQ1Mg==   \n",
       "3  81483877  MDEwOlJlcG9zaXRvcnk4MTQ4Mzg3Nw==   \n",
       "4  38904647  MDEwOlJlcG9zaXRvcnkzODkwNDY0Nw==   \n",
       "\n",
       "                              name                              full_name  \\\n",
       "0  alexa-skills-kit-sdk-for-nodejs  alexa/alexa-skills-kit-sdk-for-nodejs   \n",
       "1                   alexa-cookbook                   alexa/alexa-cookbook   \n",
       "2         skill-sample-nodejs-fact         alexa/skill-sample-nodejs-fact   \n",
       "3                   avs-device-sdk                   alexa/avs-device-sdk   \n",
       "4    alexa-skills-kit-sdk-for-java    alexa/alexa-skills-kit-sdk-for-java   \n",
       "\n",
       "   private                                              owner  \\\n",
       "0    False  {'login': 'alexa', 'id': 17815977, 'node_id': ...   \n",
       "1    False  {'login': 'alexa', 'id': 17815977, 'node_id': ...   \n",
       "2    False  {'login': 'alexa', 'id': 17815977, 'node_id': ...   \n",
       "3    False  {'login': 'alexa', 'id': 17815977, 'node_id': ...   \n",
       "4    False  {'login': 'alexa', 'id': 17815977, 'node_id': ...   \n",
       "\n",
       "                                            html_url  \\\n",
       "0  https://github.com/alexa/alexa-skills-kit-sdk-...   \n",
       "1            https://github.com/alexa/alexa-cookbook   \n",
       "2  https://github.com/alexa/skill-sample-nodejs-fact   \n",
       "3            https://github.com/alexa/avs-device-sdk   \n",
       "4  https://github.com/alexa/alexa-skills-kit-sdk-...   \n",
       "\n",
       "                                         description   fork  \\\n",
       "0  The Alexa Skills Kit SDK for Node.js helps you...  False   \n",
       "1  A series of sample code projects to be used fo...  False   \n",
       "2                          Build An Alexa Fact Skill  False   \n",
       "3  An SDK for commercial device makers to integra...  False   \n",
       "4  The Alexa Skills Kit SDK for Java helps you ge...  False   \n",
       "\n",
       "                                                 url  ... disabled  \\\n",
       "0  https://api.github.com/repos/alexa/alexa-skill...  ...    False   \n",
       "1  https://api.github.com/repos/alexa/alexa-cookbook  ...    False   \n",
       "2  https://api.github.com/repos/alexa/skill-sampl...  ...    False   \n",
       "3  https://api.github.com/repos/alexa/avs-device-sdk  ...    False   \n",
       "4  https://api.github.com/repos/alexa/alexa-skill...  ...    False   \n",
       "\n",
       "  open_issues_count                                            license forks  \\\n",
       "0                 8  {'key': 'apache-2.0', 'name': 'Apache License ...   670   \n",
       "1                13  {'key': 'other', 'name': 'Other', 'spdx_id': '...   912   \n",
       "2                 7  {'key': 'apache-2.0', 'name': 'Apache License ...  1186   \n",
       "3                54  {'key': 'apache-2.0', 'name': 'Apache License ...   477   \n",
       "4                 2  {'key': 'apache-2.0', 'name': 'Apache License ...   720   \n",
       "\n",
       "  open_issues watchers default_branch  \\\n",
       "0           8     2811          2.0.x   \n",
       "1          13     1557         master   \n",
       "2           7     1002         master   \n",
       "3          54      993         master   \n",
       "4           2      715          2.0.x   \n",
       "\n",
       "                                     permissions score  \\\n",
       "0  {'admin': False, 'push': False, 'pull': True}     1   \n",
       "1  {'admin': False, 'push': False, 'pull': True}     1   \n",
       "2  {'admin': False, 'push': False, 'pull': True}     1   \n",
       "3  {'admin': False, 'push': False, 'pull': True}     1   \n",
       "4  {'admin': False, 'push': False, 'pull': True}     1   \n",
       "\n",
       "                                              readme  \n",
       "0  <p align=\"center\">\\n  <img src=\"https://m.medi...  \n",
       "1  \\n# Alexa Skill Building Cookbook\\n\\n<div styl...  \n",
       "2  # Build An Alexa Fact Skill\\n<img src=\"https:/...  \n",
       "3  ### What is the Alexa Voice Service (AVS)?\\n\\n...  \n",
       "4  <p align=\"center\">\\n  <img src=\"https://m.medi...  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missed = 0\n",
    "\n",
    "def add_readme(x, missed):\n",
    "    \"\"\"Use the README dict to add the README for this full_name\"\"\"\n",
    "    try:\n",
    "        return readmes[x['full_name']]\n",
    "    except KeyError:\n",
    "        missed +=1\n",
    "        return \"\"\n",
    "\n",
    "df['readme'] = df.apply(lambda x: add_readme(x, missed), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the Data for Hand Labeling of a Sample\n",
    "\n",
    "Store the data as CSV for hand labeling to guide our Labeling Function development. Also store to Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'node_id', 'name', 'full_name', 'private', 'owner', 'html_url',\n",
       "       'description', 'fork', 'url', 'forks_url', 'keys_url',\n",
       "       'collaborators_url', 'teams_url', 'hooks_url', 'issue_events_url',\n",
       "       'events_url', 'assignees_url', 'branches_url', 'tags_url', 'blobs_url',\n",
       "       'git_tags_url', 'git_refs_url', 'trees_url', 'statuses_url',\n",
       "       'languages_url', 'stargazers_url', 'contributors_url',\n",
       "       'subscribers_url', 'subscription_url', 'commits_url', 'git_commits_url',\n",
       "       'comments_url', 'issue_comment_url', 'contents_url', 'compare_url',\n",
       "       'merges_url', 'archive_url', 'downloads_url', 'issues_url', 'pulls_url',\n",
       "       'milestones_url', 'notifications_url', 'labels_url', 'releases_url',\n",
       "       'deployments_url', 'created_at', 'updated_at', 'pushed_at', 'git_url',\n",
       "       'ssh_url', 'clone_url', 'svn_url', 'homepage', 'size',\n",
       "       'stargazers_count', 'watchers_count', 'language', 'has_issues',\n",
       "       'has_projects', 'has_downloads', 'has_wiki', 'has_pages', 'forks_count',\n",
       "       'mirror_url', 'archived', 'disabled', 'open_issues_count', 'license',\n",
       "       'forks', 'open_issues', 'watchers', 'default_branch', 'permissions',\n",
       "       'score', 'readme'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the Data Out to Parquet\n",
    "\n",
    "Here we flatten the nested fields we need, remove the rest and store the data in Parquet format for later use. We also create a dummy/empty `label` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow\n",
    "\n",
    "# # Show all columns\n",
    "# pd.options.display.max_columns = None\n",
    "\n",
    "# # Un-nest/flatten owner and license\n",
    "# df['owner']   = df['owner'].apply(lambda x:  x['login'] if isinstance(x, dict) and 'login' in x else x)\n",
    "# df['license'] = df['license'].apply(lambda x: x['name'] if isinstance(x, dict) and 'name'  in x else x)\n",
    "\n",
    "# # Write the relevant columns to parquet format\n",
    "# df[[\n",
    "#     'id',\n",
    "#     'node_id',\n",
    "#     'name',\n",
    "#     'full_name',\n",
    "#     'owner',\n",
    "#     'description',\n",
    "#     'url',\n",
    "#     'html_url',\n",
    "#     'created_at',\n",
    "#     'updated_at',\n",
    "#     'pushed_at',\n",
    "#     'homepage',\n",
    "#     'size',\n",
    "#     'stargazers_count',\n",
    "#     'watchers_count',\n",
    "#     'language',\n",
    "#     'license',\n",
    "#     'forks_count',\n",
    "#     'open_issues_count',\n",
    "#     'default_branch',\n",
    "#     'score',\n",
    "#     'readme',\n",
    "# ]].to_parquet(\n",
    "#     'data/Amazon.Repos.READMEs.2-26-2020.parquet'\n",
    "# )\n",
    "\n",
    "# # Load the data we previously enriched with Github READMEs\n",
    "# df = pd.read_parquet('data/Amazon.Repos.READMEs.2-26-2020.parquet')\n",
    "\n",
    "# Add a dummy column for labels\n",
    "df['label'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Hand Labeling\n",
    "\n",
    "Here we prepare the data for labeling using [SMART](https://github.com/RTIInternational/SMART). I had to alter smart with this [pull request](https://github.com/RTIInternational/SMART/pull/47) (explanation in [ticket #46](https://github.com/RTIInternational/SMART/issues/46)) to display multi-line project descriptions and annotation cards. This way we can show the project URL, description and the top part of the README in a readable way.\n",
    "\n",
    "![Smart Project](images/SMART_multi_line_project_description.png)\n",
    "\n",
    "![Smart Annotation Card](images/SMART_multi_line_annotation_card.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "\n",
    "\n",
    "def extract_text_plain(x):\n",
    "    \"\"\"Extract non-code text from posts (questions/answers)\"\"\"\n",
    "    if not isinstance(x, str):\n",
    "        x = ''\n",
    "    doc = BeautifulSoup(x or '')\n",
    "    codes = doc.find_all('code')\n",
    "    [code.extract() if code else None for code in codes]\n",
    "    text = re.sub(r'http\\S+', ' ', doc.text)\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Add the README text to the dataframe\n",
    "df['readme_text'] = df['readme'].apply(extract_text_plain)\n",
    "\n",
    "\n",
    "def create_question(row):\n",
    "    \"\"\"Given a row in a DataFrame, add a question column from the full_name and description\"\"\"\n",
    "    \n",
    "    head_len = 3000\n",
    "    readme_len = len(row['readme_text'])\n",
    "    if readme_len < head_len:\n",
    "        head_len = readme_len\n",
    "    readme_text = row['readme_text'][:head_len]\n",
    "    \n",
    "    question = (\n",
    "        f\"\"\"{row['url']}\n",
    "\n",
    "{row['description']}\n",
    "\n",
    "{readme_text}\"\"\"\n",
    "    )\n",
    "    row['question'] = question\n",
    "    return row\n",
    "\n",
    "\n",
    "# Select and rename the fields to the format that SMART expects. See https://github.com/RTIInternational/SMART\n",
    "df_smart = df.apply(create_question, axis=1)[['id', 'question', 'label']]\n",
    "df_smart = df_smart.rename(columns={'id': 'ID', 'question': 'Text', 'label': 'Label'})\n",
    "\n",
    "# Write to CSV for SMART to load for hand labeling.\n",
    "df_smart.to_csv(\n",
    "    'data/Amazon_Open_Source_Analysis_Questions - SMART.csv',\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the READMEs\n",
    "\n",
    "We now load the READMEs back and join it with the original repository DataFrame.\n",
    "\n",
    "**Note: From now on we will simply load the data, skipping the previous cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary load from other machine\n",
    "import pyarrow\n",
    "\n",
    "readme_df = pd.read_parquet('data/aws_readmes.parquet', engine='pyarrow')[['id', 'readme']]\n",
    "\n",
    "# Join READMEs in and drop duplicate ID column\n",
    "df_join = df.join(readme_df, lsuffix='_readme_df')\n",
    "del df_join['id_readme_df']\n",
    "\n",
    "df = df_join\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spaCy Documents from READMEs\n",
    "\n",
    "Setup the large english language model and have it merge multi-token named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.pipeline import merge_entities\n",
    "\n",
    "\n",
    "# Enable a GPU if you have one\n",
    "spacy.prefer_gpu()\n",
    "\n",
    "# Download the spaCy english model\n",
    "spacy.cli.download('en_core_web_lg')\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Merge multi-token entities together\n",
    "nlp.add_pipe(merge_entities)\n",
    "\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spacy'] = df['readme'].apply(nlp)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Gold Labeled Data\n",
    "\n",
    "Data was labeled via a [Google Sheet](https://docs.google.com/spreadsheets/d/1ULt0KxIdb5HUJCEMt_AmOuPbTvN1zg8UA_4RvjlVwXQ/edit?usp=sharing) and exported to CSV at [data/Amazon_Open_Source_Analysis_Gold.csv](data/Amazon_Open_Source_Analysis_Gold.csv).\n",
    "\n",
    "### Note: Submitting Corrections or Additions\n",
    "\n",
    "If you feel any labels are wrong, first read the definitions in the README and comment on the sheet. You may also copy the Google Sheet and continue labeling yourself if you want to ensure the accuracy of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 2,469 records and then filter out the unlabeled ones (all but 200)\n",
    "df_gold = pd.read_csv('data/Amazon_Open_Source_Analysis_Gold.csv')\n",
    "\n",
    "df_gold = df_gold[df_gold['label'].notnull()]\n",
    "print(f'Gold labeled records: {len(df_gold.index):,}')\n",
    "\n",
    "df_gold = df.set_index('id').join(\n",
    "    df_gold.set_index('id'),\n",
    "    how='inner',\n",
    "    on='id',\n",
    "    rsuffix='_gold',\n",
    ")\n",
    "\n",
    "# Drop duplicate columns\n",
    "df_gold = df_gold.drop(\n",
    "    [\n",
    "        'full_name_gold','url_gold','description_gold','fork_gold','forks_count_gold',\n",
    "        'language_gold','homepage_gold','open_issues_count_gold','watchers_gold', \n",
    "        'readme_gold',\n",
    "        \n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Drop ABSTAIN labels\n",
    "df_gold = df_gold[df_gold['label'] != 'ABSTAIN']\n",
    "print(f'Records minus ABSTAIN: {len(df_gold.index):,}')\n",
    "\n",
    "df_gold[['full_name', 'label']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Label Imbalance\n",
    "\n",
    "If the labels are highly imbalanced, it will throw off our `LabelModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Label Schema\n",
    "\n",
    "The labels for this dataset are:\n",
    "\n",
    "| Number | Code      | Description                       |\n",
    "|--------|-----------|-----------------------------------|\n",
    "| -1     | ABSTAIN   | No vote, for Labeling Functions   |\n",
    "| 0      | GENERAL   | A FOSS project of general appeal  |\n",
    "| 1      | API       | An API library for AWS            |\n",
    "| 2      | EDUCATION | An educational library for AWS    |\n",
    "| 3      | DATASET   | REMOVED: An open dataset by Amazon|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTAIN   = -1\n",
    "GENERAL   = 0\n",
    "API       = 1\n",
    "EDUCATION = 2\n",
    "# DATASET   = 3\n",
    "\n",
    "label_pairs = [\n",
    "    (ABSTAIN, 'ABSTAIN'),\n",
    "    (GENERAL, 'GENERAL'),\n",
    "    (API, 'API'),\n",
    "    (EDUCATION, 'EDUCATION'),\n",
    "    # (DATASET, 'DATASET'),\n",
    "]\n",
    "\n",
    "# Forward and reverse indexes to labels/names\n",
    "number_to_name_dict = dict(label_pairs)\n",
    "name_to_number_dict = dict([(x[1],x[0]) for x in label_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the labels to their numeric label numbers\n",
    "def name_to_number(name):\n",
    "    \"\"\"Convert string labels from the Google Sheet to their numeric values\"\"\"\n",
    "    return name_to_number_dict[name]\n",
    "\n",
    "\n",
    "def number_to_name(num):\n",
    "    \"\"\"Convert numeric labels to their values in the Google Sheet\"\"\"\n",
    "    return number_to_name_dict[num]\n",
    "\n",
    "\n",
    "df_gold['label_num'] = df_gold['label'].apply(name_to_number)\n",
    "df_gold[['full_name','label','label_num']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Create a Random Forest Model using a Sparse Representation to Pick Keyword Label Functions\n",
    "\n",
    "We will use the spaCy doc we created to lemmatize as we tokenize the words, giving us better representations for feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "\n",
    "STOP_WORDS = STOP_WORDS.union(set(['-', '=', \"'\", '/', '.', ';', '#', '##', '###', '<', '>', ':', '\\n', '\\n\\n', '!', '[', ']', ')', '{', '}',]))\n",
    "\n",
    "\n",
    "def tokenize(doc):\n",
    "    \"\"\"Tokenize: skip stop words, return proper nouns as n-grams and lemmas for everything else\"\"\"\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Drop stop words\n",
    "        if token.text.lower() in STOP_WORDS:\n",
    "            continue\n",
    "            \n",
    "        # Lemmatize anything that isn't a proper noun\n",
    "        if token.pos_ != 'PROPN':\n",
    "            tokens.append(token.lemma_.lower())\n",
    "        else:\n",
    "            tokens.append(token.text.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df_gold['lemmas'] = df_gold['spacy'].apply(tokenize)\n",
    "df_gold['lemmas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorize and Split the Text Data and Labels into Train/Test Sets\n",
    "\n",
    "We need to vectorize the data in a sparse representation to train the model and get feature importances for each word, so we use sklearn's `TfidfVectorizer` to give more important words more weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    min_df=3,\n",
    "    stop_words=None,\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    token_pattern=None,\n",
    "    lowercase=False,\n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "df_gold_train, df_gold_test, train_labels, test_labels = train_test_split(\n",
    "    df_gold,\n",
    "    df_gold['label_num'],\n",
    "    test_size=0.3,\n",
    "    random_state=1337,\n",
    ")\n",
    "\n",
    "train_vec = vectorizer.fit_transform(\n",
    "    df_gold_train['lemmas']\n",
    ")\n",
    "test_vec = vectorizer.transform(\n",
    "    df_gold_test['lemmas']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Train a `RandomForestClassifier` and Determine Overall Feature Importances\n",
    "\n",
    "A random forest model can give us overall feature importances directly, but it doesn't tell us which class they were important for or in which direction: for or against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "# Fit the model on the training data\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_vec, train_labels)\n",
    "\n",
    "# Score the model to see if it is worth using for inference\n",
    "avg = 'weighted'\n",
    "pred = clf.predict(test_vec)\n",
    "print(f\"Model weighted F1 score: {f1_score(test_labels, pred, average=avg)}\")\n",
    "\n",
    "# Display features and importances in a DataFrame\n",
    "features = pd.DataFrame(\n",
    "    {'importance': clf.feature_importances_},\n",
    "    index=vectorizer.get_feature_names(),\n",
    ")\n",
    "features = features.sort_values(\n",
    "    by=['importance'],\n",
    "    ascending=False,\n",
    ")\n",
    "features[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data to Enable and Experiment\n",
    "\n",
    "We need test, train and development sets to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the common notation for df_train and df_test\n",
    "df_train = df\n",
    "y_train = np.zeros(\n",
    "    len(df.index)\n",
    ")\n",
    "df_test = df_gold\n",
    "y_test  = df_gold['label_num'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Utilities for Generating Keyword `LabelFunctions`\n",
    "\n",
    "Given a set of keywords and a set of fields, we want to generate a set of those keywords operating on each of the fields. We want the list of keywords to be together and the list of fields to be separate `LabelFunctions`, although we may or may not actually group keywords in a single LF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelingFunction\n",
    "\n",
    "\n",
    "def keyword_lookup(x, keywords, field, label):\n",
    "    \"\"\"Given a list of tuples, look for any of a list of keywords\"\"\"\n",
    "    if field in x and x[field] and any(word.lower() in x[field].lower() for word in keywords):\n",
    "        return label\n",
    "    return ABSTAIN\n",
    "\n",
    "\n",
    "def make_keyword_lf(keywords, field, label=ABSTAIN, separate=False):\n",
    "    \"\"\"Given a list of keywords and a label, return a keyword search LabelingFunction\"\"\"\n",
    "    prefix = 'separate_' if separate else ''\n",
    "    name = f'{prefix}{keywords[0]}_field_{field}'        \n",
    "    return LabelingFunction(\n",
    "        name=name,\n",
    "        f=keyword_lookup,\n",
    "        resources=dict(\n",
    "            keywords=keywords,\n",
    "            field=field,\n",
    "            label=label,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def make_keyword_lfs(keywords, fields, label=ABSTAIN, separate=False):\n",
    "    \"\"\"Given a list of keywords and fields, make one or more LabelingFunctions for the keywords with each field\"\"\"\n",
    "    lfs = []\n",
    "    for field in fields:\n",
    "        \n",
    "        # Optionally make one LF per keyword\n",
    "        if separate:\n",
    "            for i, keyword in enumerate(keywords):\n",
    "                lfs.append(\n",
    "                    make_keyword_lf(\n",
    "                        [keyword],\n",
    "                        field,\n",
    "                        label=label,\n",
    "                        separate=separate,\n",
    "                    )\n",
    "                )\n",
    "        # Optionally group keywords in a single LF for each field\n",
    "        else:\n",
    "            lfs.append(\n",
    "                make_keyword_lf(\n",
    "                    keywords,\n",
    "                    field,\n",
    "                    label=label,\n",
    "                )\n",
    "            )\n",
    "    return lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Schema\n",
    "\n",
    "The labels for this dataset are:\n",
    "\n",
    "| Number | Code      | Description                       |\n",
    "|--------|-----------|-----------------------------------|\n",
    "| -1     | ABSTAIN   | No vote, for Labeling Functions   |\n",
    "| 0      | GENERAL   | A FOSS project of general appeal  |\n",
    "| 1      | API       | An API library for AWS            |\n",
    "| 2      | EDUCATION | An educational library for AWS    |\n",
    "| 3      | DATASET   | REMOVED: An open dataset by Amazon|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Functions\n",
    "\n",
    "Labeling functions each weakly label the data and need only be better than random. Snorkel's\n",
    "unsupervised generative graphical model combines these weak labels into strong labels by \n",
    "looking at the overlap, conflict and coverage of each weak label set.\n",
    "\n",
    "| Logic                           | Fields                               | Label       | 200 Sample Accuracy |\n",
    "|---------------------------------|--------------------------------------|-------------|---------------------|\n",
    "| If 'sdk' is in                  | `full_name`, `description`, `readme` | `API`       |                     |\n",
    "| If 'sample' is in               | `full_name`, `description`, `readme` | `EDUCATION` |                     |\n",
    "| If 'dataset' is in              | `full_name`, `description`, `readme` | `GENERAL`   |                     |\n",
    "| If 'demonstrate' / 'demo' is in | `full_name`, `description`, `readme` | `EDUCATION` |                     |\n",
    "| If 'walkthrough' is in          | `full_name`, `description`, `readme` | `EDUCATION` |                     |\n",
    "| If 'skill' is in                | `full_name`, `description`           | `EDUCATION` |                     |\n",
    "| If 'kit' is in                  | `full_name`, `description`           | `EDUCATION` |                     |\n",
    "| If 'toolbox' is in              | `description`                        | `GENERAL`   |                     |\n",
    "| if 'extension' is in            | `description`                        | `API`       |                     |\n",
    "| id 'add amazon' is in           | `description`                        | `API`       |                     |\n",
    "| if 'integrate' is in            | `description`                        | `API`       |                     |\n",
    "| if 'ion' is in                  | `full_name`                          | `GENERAL`   |                     |\n",
    "|                                 |                                      |             |                     |\n",
    "|                                 |                                      |             |                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it says SDK, it is probably an API library\n",
    "sdk_lfs = make_keyword_lfs(\n",
    "    keywords=['sdk'],\n",
    "    fields=['full_name'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "# If api is in the name... its an API project\n",
    "api_lfs = make_keyword_lfs(\n",
    "    keywords=['api'],\n",
    "    fields=['full_name'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "# Walkthroughs be EDUCATION\n",
    "walkthrough_lfs = make_keyword_lfs(\n",
    "    keywords=['walkthrough'],\n",
    "    fields=['full_name', 'description', 'readme'],\n",
    "    label=EDUCATION,\n",
    ")\n",
    "\n",
    "# Anything mentioning a skill is usually an Alexa skill example, of which there are many\n",
    "skill_lfs = make_keyword_lfs(\n",
    "    keywords=['skill', 'skills'],\n",
    "    fields=['full_name', 'description', 'readme'],\n",
    "    label=EDUCATION,\n",
    ")\n",
    "\n",
    "# Kits be EDUCATION\n",
    "kit_lfs = make_keyword_lfs(\n",
    "    keywords=['kit', 'kits'],\n",
    "    fields=['description', 'readme'],\n",
    "    label=EDUCATION,\n",
    ")\n",
    "\n",
    "# Toolboxes are generally GENERAL\n",
    "tool_lfs = make_keyword_lfs(\n",
    "    keywords=['toolbox'],\n",
    "    fields=['description'],\n",
    "    label=GENERAL,\n",
    ")\n",
    "\n",
    "# Extensions are APIs\n",
    "extension_lfs = make_keyword_lfs(\n",
    "    keywords=['extension'],\n",
    "    fields=['description', 'readme'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "# Add amazon means API\n",
    "add_amazon_lfs = make_keyword_lfs(\n",
    "    keywords=['add amazon'],\n",
    "    fields=['description', 'readme'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "# Add amazon means API\n",
    "aws_lfs = make_keyword_lfs(\n",
    "    keywords=['aws'],\n",
    "    fields=['full_name', 'description'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "# Integrations tend to be about APIs\n",
    "integration_lfs = make_keyword_lfs(\n",
    "    keywords=['integrate', 'integration'],\n",
    "    fields=['full_name', 'description', 'readme'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "# Ion is a major GENERAL purpose project\n",
    "ion_lfs = make_keyword_lfs(\n",
    "    keywords=['ion'],\n",
    "    fields=['full_name'],\n",
    "    label=GENERAL,\n",
    ")\n",
    "\n",
    "# Sample tends to indicate EDUCATION\n",
    "sample_lfs = make_keyword_lfs(\n",
    "    keywords=['sample'],\n",
    "    fields=['full_name', 'description', 'readme'],\n",
    "    label=EDUCATION,\n",
    ")\n",
    "\n",
    "# Datasets tend to self describe themselves :)\n",
    "dataset_lfs = make_keyword_lfs(\n",
    "    keywords=['dataset'],\n",
    "    fields=['full_name', 'description'],\n",
    "    label=GENERAL,\n",
    ")\n",
    "\n",
    "# Demos be EDUCATION\n",
    "demo_lfs = make_keyword_lfs(\n",
    "    keywords=['demonstrate', 'demo'],\n",
    "    fields=['full_name', 'description', 'readme'],\n",
    "    label=EDUCATION,\n",
    ")\n",
    "\n",
    "\n",
    "# Add the LFs to one large list\n",
    "lfs = sdk_lfs + \\\n",
    "      api_lfs + \\\n",
    "      walkthrough_lfs + \\\n",
    "      skill_lfs + \\\n",
    "      kit_lfs + \\\n",
    "      tool_lfs + \\\n",
    "      extension_lfs + \\\n",
    "      add_amazon_lfs + \\\n",
    "      aws_lfs + \\\n",
    "      integration_lfs + \\\n",
    "      ion_lfs + \\\n",
    "      sample_lfs + \\\n",
    "      dataset_lfs + \\\n",
    "      demo_lfs\n",
    "lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Label Functions to the Data\n",
    "\n",
    "Since we're using Pandas we'll use `PandasLFApplier` to run the label functions over the train and test sets. The training data will be used to traing a label model while the test set will be used both for development (seeing how our label functions do) and evaluating the label model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "\n",
    "applier  = PandasLFApplier(lfs=lfs)\n",
    "L_train  = applier.apply(df=df_train)\n",
    "L_test   = applier.apply(df=df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Overall Label Coverage\n",
    "\n",
    "We need to check how much of the data is covered by our different labelers in aggregate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_label_frequency(L):\n",
    "    plt.hist(\n",
    "        (L != ABSTAIN).sum(axis=1),\n",
    "        density=True,\n",
    "        bins=range(L.shape[1])\n",
    "    )\n",
    "    plt.xlabel(\"Number of labels\")\n",
    "    plt.ylabel(\"Fraction of dataset\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_label_frequency(L_train)\n",
    "plot_label_frequency(L_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The overall label coverage looks good. Now we need to look at each label's coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Labeling Functions' Performance\n",
    "\n",
    "Overall label coverage is good but we need to make sure the distribution of our LF output is approximately even otherwise the label model won't have enough data with which to make good inferences about how LFs relate.\n",
    "\n",
    "To help with this, we first prepare a `DataFrame` of label function names and their corresponding text labels to add to the `LFAnalysis.lf_summary` output to make it clearer what the coverage is for each label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a name/label DataFrame to join to the LF Summary DataFrame below\n",
    "lf_names = [lf.name for lf in lfs]\n",
    "lf_labels = [lf._resources['label'] for lf in lfs]\n",
    "lf_label_names = [{'Labels': number_to_name_dict[l]} for l in lf_labels]\n",
    "label_name_df = pd.DataFrame(lf_label_names, index=lf_names)\n",
    "len(label_name_df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our `LFAnalysis` with Labels\n",
    "\n",
    "Now we can run the LFAnalysis, get the summary and join our label names to see a clear indication of how well we're covering each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "# Run the LF analysis on the gold labeled data\n",
    "lfa = LFAnalysis(L=L_test, lfs=lfs)\n",
    "lfa_df = lfa.lf_summary(Y=y_test)\n",
    "\n",
    "# Join the label names in because the 'Polarity' field is confusing\n",
    "lfa_label_df = lfa_df.join(label_name_df)\n",
    "lfa_label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Coverage by Label\n",
    "\n",
    "Now we can group by the label and determine the raw count correct, incorrect and total. This will give us an idea of coverage per label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lf_df = lfa_label_df.groupby('Labels').agg({'Correct': 'sum', 'Incorrect': 'sum'})\n",
    "g_lf_df['Total LFs'] = g_lf_df['Correct'] + g_lf_df['Incorrect']\n",
    "sum_total_lf = g_lf_df['Total LFs'].sum()\n",
    "g_lf_df['Total LF Ratio'] = g_lf_df['Total LFs'] / sum_total_lf\n",
    "g_lf_df['Total Correct Ratio'] = g_lf_df['Correct'] / sum_total_lf\n",
    "\n",
    "g_lf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to Gold Label Coverage\n",
    "\n",
    "We need to compare this LF coverage to the raw gold label coverage, which will give us an idea of the disparity between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_df_gold = df_gold.groupby('label').agg({'name': 'count'})\n",
    "\n",
    "sum_total_labels = g_df_gold['name'].sum()\n",
    "g_df_gold['Total Gold Label Ratio'] = g_df_gold['name'] / sum_total_labels\n",
    "\n",
    "g_df_gold.columns = ['Total Labels', 'Total Gold Label Ratio']\n",
    "g_df_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Stir\n",
    "\n",
    "Now combine them to see the disparity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = g_df_gold.join(g_lf_df)\n",
    "combined_df['LF / Label Ratio'] = combined_df['Total LF Ratio'] / combined_df['Total Gold Label Ratio']\n",
    "combined_df['Correct LF / Label Ratio'] = combined_df['Total Correct Ratio'] / combined_df['Total Gold Label Ratio']\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "It looks like we are way over-covering `EDUCATION` and way under covering `API` and to a lesser extent (by magnitude) `GENERAL`. Lets fix that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Standout Tokens Per Class\n",
    "\n",
    "Inspect the top 5 tokens in terms of TF-IDF per record then look for standouts per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "v = TfidfVectorizer()\n",
    "m = v.fit_transform(df_gold['readme'])\n",
    "index_to_word = dict([(value, key) for key, value in v.vocabulary_.items()])\n",
    "\n",
    "term_rows = []\n",
    "for row in m.toarray():\n",
    "    words = []\n",
    "    for i, val in enumerate(row):\n",
    "        if val > 0:\n",
    "            words.append((\n",
    "                val,\n",
    "                index_to_word[i]\n",
    "            ))\n",
    "    term_rows.append(\n",
    "        [y[1] for y in sorted(words, key=lambda x: x[0], reverse=True)[0:5]]\n",
    "    )\n",
    "\n",
    "df_gold['top_terms'] = term_rows\n",
    "df_short = df_gold[['full_name', 'description', 'label', 'top_terms', 'readme']]\n",
    "api_df = df_short[df_short['label'] == 'API']\n",
    "\n",
    "# See the READMEs\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "api_df[['full_name', 'description', 'label', 'top_terms', 'readme']]\n",
    "\n",
    "api_df.to_csv('/home/rjurney/api_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New LFs for API\n",
    "\n",
    "We need some new LFs for API, so lets look at common n-grams of consequence for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lfs = make_keyword_lfs(\n",
    "    keywords=['aws sdk'],\n",
    "    fields=['description', 'readme'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "alexa_api_lfs = make_keyword_lfs(\n",
    "    keywords=['alexa api'],\n",
    "    fields=['description', 'readme'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "skills_kit_lfs = make_keyword_lfs(\n",
    "    keywords=['alexa-skills-kit', 'skills-kit'],\n",
    "    fields=['full_name'],\n",
    "    label=API,\n",
    ")\n",
    "\n",
    "skill_sample_lfs = make_keyword_lfs(\n",
    "    keywords=['skill-sample'],\n",
    "    fields=['full_name'],\n",
    "    label=EDUCATION,\n",
    ")\n",
    "\n",
    "workshop_title_lfs = make_keyword_lfs(\n",
    "    keywords=['workshop'],\n",
    "    fields=['full_name'],\n",
    "    label=EDUCATION,\n",
    ")\n",
    "\n",
    "new_lfs = lfs + \\\n",
    "    sample_lfs + \\\n",
    "    alexa_api_lfs + \\\n",
    "    skills_kit_lfs + \\\n",
    "    skill_sample_lfs\n",
    "\n",
    "applier  = PandasLFApplier(lfs=new_lfs)\n",
    "L_train  = applier.apply(df=df_train)\n",
    "L_test   = applier.apply(df=df_test)\n",
    "\n",
    "lfa = LFAnalysis(L=L_test, lfs=new_lfs)\n",
    "lfa_df = lfa.lf_summary(Y=y_test)\n",
    "\n",
    "# Prepare a name/label DataFrame to join to the LF Summary DataFrame below\n",
    "lf_names = [lf.name for lf in new_lfs]\n",
    "lf_labels = [lf._resources['label'] for lf in new_lfs]\n",
    "lf_label_names = [{'Labels': number_to_name_dict[l]} for l in lf_labels]\n",
    "label_name_df = pd.DataFrame(lf_label_names, index=lf_names)\n",
    "\n",
    "lfa_label_df = lfa_df.join(label_name_df)\n",
    "lfa_label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Label Voter Baseline\n",
    "\n",
    "A simple baseline is helpful to evaluate the performance of the `LabelModel` we're about to train. We can use a majority vote labeler to label the data for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import MajorityLabelVoter\n",
    "\n",
    "\n",
    "majority_model = MajorityLabelVoter(cardinality=4)\n",
    "preds_train = majority_model.predict(\n",
    "    L=L_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative LabelModel\n",
    "\n",
    "Snorkel's generative `LabelModel` learns the relationships between labelers and creates a better label out of all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import LabelModel\n",
    "\n",
    "\n",
    "label_model = LabelModel(cardinality=4, verbose=True)\n",
    "label_model.fit(\n",
    "    L_train=L_train,\n",
    "    n_epochs=500,\n",
    "    lr=0.001,\n",
    "    log_freq=100,\n",
    "    seed=31337\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_acc = majority_model.score(L=L_test, Y=y_test, tie_break_policy='random')[\n",
    "    'accuracy'\n",
    "]\n",
    "print(f\"{'Majority Vote Accuracy:':<25} {majority_acc * 100:.1f}%\")\n",
    "\n",
    "label_model_acc = label_model.score(L=L_test, Y=y_test, tie_break_policy='random')[\n",
    "    'accuracy'\n",
    "]\n",
    "print(f\"{'Label Model Accuracy:':<25} {label_model_acc * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Errors to Improve `LabelFunctions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.analysis import get_label_buckets\n",
    "\n",
    "\n",
    "# Trim the fields for figuring out problems\n",
    "df_viz = df_test[['full_name', 'description', 'label']]\n",
    "\n",
    "# Display all errors for debugging purposes\n",
    "pd.set_option('display.max_rows', len(df_viz.index))\n",
    "\n",
    "\n",
    "def get_mistakes(df, probs_test, buckets, labels, label_names):\n",
    "    \"\"\"Take DataFrame and pair of actual/predicted labels/names and return a DataFrame showing those records.\"\"\"\n",
    "    df_fn = df.iloc[buckets[labels]]\n",
    "    df_fn['probability'] = probs_test[buckets[labels], 1]\n",
    "    df_fn['true label'] = label_names[0]\n",
    "    df_fn['predicted label'] = label_names[1]\n",
    "    return df_fn\n",
    "\n",
    "\n",
    "def mistakes_df(df, label_model, L_test, y_test):\n",
    "    \"\"\"Compute a DataFrame of all the mistakes we've seen.\"\"\"\n",
    "    out_dfs = []\n",
    "\n",
    "    probs_test = label_model.predict_proba(L=L_test)\n",
    "    preds_test = probs_test >= 0.5\n",
    "\n",
    "    buckets = get_label_buckets(\n",
    "        y_test,\n",
    "        L_test[:, 1]\n",
    "    )\n",
    "    print(buckets)\n",
    "\n",
    "    for (actual, predicted) in buckets.keys():\n",
    "    \n",
    "        # Only shot mistakes that we actually voted on\n",
    "        if actual != predicted:\n",
    "\n",
    "            actual_name    = number_to_name_dict[actual]\n",
    "            predicted_name = number_to_name_dict[predicted]\n",
    "\n",
    "            out_dfs.append(\n",
    "                get_mistakes(\n",
    "                    df,\n",
    "                    probs_test,\n",
    "                    buckets=buckets,\n",
    "                    labels=(actual, predicted),\n",
    "                    label_names=(actual_name, predicted_name)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    if len(out_dfs) > 1:    \n",
    "        return out_dfs[0].append(\n",
    "            out_dfs[1:]\n",
    "        )\n",
    "    else:\n",
    "        return out_dfs[0]\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "m_df = mistakes_df(df_viz, majority_model, L_test, y_test)\n",
    "m_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_df_voted = m_df[m_df['predicted label'] != 'ABSTAIN']\n",
    "m_df_voted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_index = [i for i in range(0,L_test.shape[0])]\n",
    "\n",
    "votes_df = pd.DataFrame(\n",
    "    data=L_test,\n",
    "    index=df_viz.index,\n",
    "    columns=lf_names,\n",
    ")\n",
    "def c(x):\n",
    "    return number_to_name_dict[x]\n",
    "\n",
    "votes_df.apply(c, axis=1)\n",
    "\n",
    "# df1 = votes_df.gt(0, 0)\n",
    "# s = df1.apply(lambda x: ', '.join(x.index[x]),axis=1)\n",
    "# df_viz['positive_votes'] = s\n",
    "# df_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out Unlabeled Data Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import filter_unlabeled_dataframe\n",
    "\n",
    "\n",
    "probs_test = label_model.predict_proba(L=L_test)\n",
    "preds_test = probs_test >= 0.5\n",
    "\n",
    "buckets = get_label_buckets(\n",
    "    y_test,\n",
    "    L_test[:, 1]\n",
    ")\n",
    "print(buckets)\n",
    "\n",
    "\n",
    "df_test_filtered, probs_test_filtered = filter_unlabeled_dataframe(\n",
    "    X=df_viz, y=probs_test, L=L_test\n",
    ")\n",
    "df_test_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_no_abstain_df = m_df[m_df['predicted label'] != 'ABSTAIN']\n",
    "len(m_no_abstain_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['full_name'].str.startswith('alexa/alexa-skills-kit-sdk')][['full_name', 'description']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at Ion libraries\n",
    "df_gold[df['full_name'].str.contains('ion')][['full_name', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
